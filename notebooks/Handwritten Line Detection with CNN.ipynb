{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Handwritten Line Detection with CNN.ipynb","provenance":[],"collapsed_sections":["BAnSuVTEaOLw","FO7bi393aVQF","P8724xlER1Ac","30W3QsK4R8vz","yebdpoH6SD5c"],"mount_file_id":"1dAekZAggBghK6KR22raC66u64mV5ebUp","authorship_tag":"ABX9TyNxX04a49Nzlc2Ob6VtWO7j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sOMs9Au_6u9J"},"source":["# Handwritten Line Detection with CNN"]},{"cell_type":"markdown","metadata":{"id":"vUrcXUWNS1lf"},"source":["Prepare Environment\r\n","- Git clone etc.\r\n","- Download the raw data\r\n","- Preprocess the data"]},{"cell_type":"code","metadata":{"id":"dIpWGsYyGCKA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607878559807,"user_tz":-180,"elapsed":990,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"dfd74099-2777-447c-8870-d1d4f9dc0d37"},"source":["%cd drive/MyDrive/PixelwiseCNN/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/PixelwiseCNN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cljluvuIGlCY","executionInfo":{"status":"ok","timestamp":1607878567804,"user_tz":-180,"elapsed":947,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"78c7ce12-7e1e-4163-b1eb-10dda9f1db91"},"source":["%ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdataset\u001b[0m/  \u001b[01;34mmodels\u001b[0m/  \u001b[01;34mshrankedDB_output\u001b[0m/  \u001b[01;34mshrankedDB_output_clipped\u001b[0m/  \u001b[01;34mweight\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4CZiW6KURDG7"},"source":["## Common Imports"]},{"cell_type":"code","metadata":{"id":"WZ-di9t0R1N_","executionInfo":{"status":"ok","timestamp":1607878569535,"user_tz":-180,"elapsed":2663,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["import os\r\n","import re\r\n","import cv2\r\n","import glob\r\n","import random\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","from timeit import default_timer as timer\r\n","from PIL import Image\r\n","from tqdm import tqdm\r\n","\r\n","import torch \r\n","import torch.nn.functional as F\r\n","from torch import nn \r\n","from torch import optim \r\n","from torch.utils.data import Dataset, DataLoader\r\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\r\n","\r\n","import torchvision \r\n","import torchvision.transforms.functional as TF\r\n","from torchvision import transforms\r\n","from torchsummary import summary\r\n","\r\n","from models.Unet_model import UnetModel\r\n","# from models.Unet_model_clipped import UnetModelClipped\r\n","\r\n","# from torchvision.models.segmentation import deeplabv3_resnet50 as resnet50"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BJb1fufKRF91"},"source":["Check Cuda Before Computation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UpaewARVHSe3","executionInfo":{"status":"ok","timestamp":1607878569539,"user_tz":-180,"elapsed":2653,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"8bc6a947-a3ae-4969-963b-3909f47502c7"},"source":["print(f'Cuda Available: {torch.cuda.is_available()}')\r\n","print(f'{\"Cuda Device Name: \" + torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"No Cuda Device Found\"}')\r\n","\r\n","# CUDA for PyTorch\r\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n","torch.backends.cudnn.benchmark = True"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Cuda Available: True\n","Cuda Device Name: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GSkjejY_RKwA"},"source":["## Hyperparameters and Data Paths"]},{"cell_type":"markdown","metadata":{"id":"1yOubphuoqNE"},"source":["### Hyperparameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qUIXP-KOosaY","executionInfo":{"status":"ok","timestamp":1607878569540,"user_tz":-180,"elapsed":2538,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"47909c75-319d-4722-8c15-165f2b195b29"},"source":["# Hyperparameters\r\n","epochs = 40               # 4 predicts well, might be 2. 8 doesn't affect much ~0.5%\r\n","batch_size = 16           # 4 is OK, might be 8 (exceed mem.)\r\n","batch_extender = False    # Extends the batch so that training process done once in twice -> gives better result\r\n","learning_rate = 1e-2      # 1e-3 is OK., 5e-4 also OK. (0.01 -> 0.001 -> 0.0005) LR Scheduler!\r\n","dropout_rate = 0.0        # 0.2 is nice with big train data\r\n","loss_print_per_epoch = 1  # desired # loss data print per epoch\r\n","number_of_classes = 2     # OK.\r\n","validation_on = True\r\n","scheduler_on = True\r\n","sample_view = False\r\n","is_saving_output = True\r\n","\r\n","# Hyperparameter Print\r\n","print(f'Epoch: {epochs}')\r\n","print(f'Dropout Rate: {dropout_rate}')\r\n","print(f'Learning Rate: {learning_rate}')\r\n","print(f'Batch Size: {batch_size*2 if batch_extender else batch_size} {\"(Artifical Batch)\" if batch_extender else \"\"}')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch: 40\n","Dropout Rate: 0.0\n","Learning Rate: 0.01\n","Batch Size: 16 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RVt-hP_Kotqe"},"source":["### Paths"]},{"cell_type":"code","metadata":{"id":"304Fctp1FiCD","executionInfo":{"status":"ok","timestamp":1607878569541,"user_tz":-180,"elapsed":2530,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["# Image Paths\n","data_dir = 'dataset'\n","train_path = data_dir + '/2_train'\n","test_path = data_dir + '/2_test'\n","validation_path = data_dir + '/2_validation'\n","\n","# Trained Model Path\n","trained_model_path = 'weight/model_check.pt'\n","os.makedirs(os.path.join(os.getcwd(), trained_model_path.split(\"/\")[0]), exist_ok=True)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adsaQqzVHpbm"},"source":["## Boilerplate Functions"]},{"cell_type":"code","metadata":{"id":"0GeYqyhrHo2W","executionInfo":{"status":"ok","timestamp":1607878569542,"user_tz":-180,"elapsed":2482,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["# Plots the given batch in 3 rows; Raw, Mask, Bitwise_Anded\r\n","def plt_images(images, masks):\r\n","    fig, axs = plt.subplots(3, batch_size, figsize=(images[0].shape))\r\n","\r\n","    for i in range(len(images)):\r\n","        axs[0][i].imshow(images[i], cmap='gray')\r\n","        axs[1][i].imshow(masks[i], cmap='gray')\r\n","        axs[2][i].imshow(images[i] & masks[i], cmap='gray')\r\n","    fig.suptitle(\"Top Row: raw images, Middle Row: masks, Bottom Row: bitwise_and masks\")\r\n","    plt.show()\r\n","\r\n","\r\n","# Returns the images and masks in the original format\r\n","def undo_preprocess(images, predicts):\r\n","    x = []\r\n","    y = []\r\n","\r\n","    images = images.cpu().numpy()\r\n","    predicts = predicts.cpu().numpy()\r\n","\r\n","    for index in range(images.shape[0]):\r\n","        image = images[index]\r\n","        # Needed to convert c,h,w -> h,w,c\r\n","        image = np.transpose(image, (1, 2, 0))\r\n","        # make every pixel 0-1 range than mul. 255 to scale the value\r\n","        image = np.squeeze(image) * 255\r\n","        x.append(image.astype(np.uint8))\r\n","\r\n","        predict = predicts[index]\r\n","        # Needed to convert c,h,w -> h,w,c\r\n","        mask_array = np.transpose(predict, (1, 2, 0))\r\n","        # Every pixel has two class grad, so we pick the highest\r\n","        mask_array = np.argmax(mask_array, axis=2) * 255\r\n","        mask_array = mask_array.astype(np.uint8)\r\n","        y.append(mask_array)\r\n","    return np.array(x), np.array(y)\r\n","\r\n","\r\n","# Saves the given batch in directory\r\n","def save_output_batch(images, outputs):\r\n","    path = os.path.join(os.getcwd(), 'output_batch/')\r\n","    os.makedirs(path, exist_ok=True)\r\n","    print(f'You can find samples in \\'{path}\\'')\r\n","\r\n","    for index in range(len(images)):\r\n","        image = images[index]\r\n","        save_image = Image.fromarray(image)\r\n","        save_image.save(path + str(index) + '_input.png')\r\n","\r\n","        mask = outputs[index]\r\n","        save_mask = Image.fromarray(mask)\r\n","        save_mask.save(path + str(index) + '_output.png')\r\n","\r\n","\r\n","# Saves the given batch in directory\r\n","def save_predictions(images, predictions):\r\n","    path = os.path.join(os.getcwd(), 'output/')\r\n","    path = os.path.join(path, 'prediction/')\r\n","    os.makedirs(path, exist_ok=True)\r\n","    for index, prediction in enumerate(predictions):\r\n","        save_prediction = Image.fromarray(prediction)\r\n","        save_prediction.save(path + str(index) + '_output.png')\r\n","    \r\n","    for index in range(len(images)):\r\n","        save_image = Image.fromarray(images[index])\r\n","        save_image.save(path + str(index) + '_input.png')\r\n","\r\n","        save_prediction = Image.fromarray(predictions[index])\r\n","        save_prediction.save(path + str(index) + '_output.png')\r\n","    print(f'You can find predictions in \\'{path}\\'')"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADMmO8C_kQDo"},"source":["## Loading Data"]},{"cell_type":"code","metadata":{"id":"D36chSGwi7It","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607880392680,"user_tz":-180,"elapsed":1825606,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"8c26281f-b183-41d2-feb6-60a2e9ce2bdb"},"source":["# Loads the data from the given path\n","def load_data(dataset_path):\n","    forms = []\n","    masks = []\n","\n","    form_names = glob.glob(dataset_path + '/form' + '/*.png') # sample path -> './dataset/train/form/*.png'\n","    form_names.sort(key=lambda f: int(re.sub('\\D', '', f))) # Sorts them as 0,1,2..\n","\n","    mask_names = glob.glob(dataset_path + '/mask' + '/*.png') # sample path -> './dataset/train/mask/*.png'\n","    mask_names.sort(key=lambda f: int(re.sub('\\D', '', f))) # Sorts them as 0,1,2..\n","\n","    for image_file_name, mask_file_name in tqdm(zip(form_names, mask_names)):\n","        image = np.asarray(Image.open(image_file_name))\n","        mask = np.asarray(Image.open(mask_file_name))\n","\n","        forms.append(image)\n","        masks.append(mask)\n","    \n","    return np.array(forms), np.array(masks)\n","\n","\n","# Load data from paths\n","train_images, train_masks = load_data(train_path) \n","test_images, test_masks = load_data(test_path) \n","validation_images, validation_masks = load_data(validation_path) "],"execution_count":11,"outputs":[{"output_type":"stream","text":["984it [19:29,  1.19s/it]\n","308it [05:51,  1.14s/it]\n","247it [04:48,  1.17s/it]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"BAnSuVTEaOLw"},"source":["## Handwriting Form Dataset"]},{"cell_type":"code","metadata":{"id":"2DdIaYz-60Cj","executionInfo":{"status":"ok","timestamp":1607880392682,"user_tz":-180,"elapsed":1825596,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["# DATASET CLASS\n","class FormDS(Dataset):\n","    def __init__(self, images, masks, number_of_classes: int, augmentation=False):\n","        self.images = images\n","        self.masks = masks\n","        self.number_of_classes = number_of_classes\n","        self.length = len(images)\n","        self.augmentation = augmentation\n","\n","    # Converts the image, a PIL image, into a PyTorch Tensor\n","    def transform(self, image, mask):\n","        # needed to apply transforms\n","        image = transforms.ToPILImage()(image)\n","        mask = transforms.ToPILImage()(mask)\n","\n","        if self.augmentation:\n","            # Random horizontal flipping\n","            if random.random() > 0.5:\n","                image = TF.hflip(image)\n","                mask = TF.hflip(mask)\n","\n","            # Random vertical flipping\n","            if random.random() > 0.5:\n","                image = TF.vflip(image)\n","                mask = TF.vflip(mask)\n","            \n","        img = TF.to_tensor(np.array(image))\n","        msk = TF.to_tensor(np.array(mask))\n","        return img, msk\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        image = image.astype(np.float32)\n","        image = image / 255  # make pixel values between 0-1\n","\n","        mask = self.masks[idx]\n","        mask = mask.astype(np.float32)\n","        mask = mask / 255   # make pixel values 0-1\n","\n","        # make each pixel to have either 0 or 1  -> will be deleted because we used Nearest while scaling\n","        mask[mask > .7] = 1\n","        mask[mask <= .7] = 0\n","\n","        image, mask = self.transform(image, mask)\n","\n","        return image, mask\n","\n","    def __len__(self):\n","        return self.length\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FO7bi393aVQF"},"source":["### Dataloader"]},{"cell_type":"code","metadata":{"id":"Uugfwo51jLhE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607880392684,"user_tz":-180,"elapsed":1825589,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"3161346b-c078-49e8-fb7a-fef3a3a216a3"},"source":["train_size = 984\n","\n","# Train Dataset Loaded to Torch Here\n","train_dataset = FormDS(train_images[:train_size], train_masks[:train_size], number_of_classes, augmentation=True)\n","train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","print(f'Train DS: Image Size: {len(train_dataset)} with {len(train_data_loader)} total batches')\n","\n","# Test Dataset Loaded to Torch Here\n","test_dataset = FormDS(test_images, test_masks, number_of_classes, augmentation=True)\n","test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","print(f'Test  DS: Image Size: {len(test_dataset)} with {len(test_data_loader)} total batches')\n","\n","# Validation Dataset Loaded to Torch Here\n","validation_dataset = FormDS(validation_images, validation_masks, number_of_classes, augmentation=True)\n","validation_data_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n","print(f'Valid DS: Image Size: {len(validation_dataset)} with {len(validation_data_loader)} total batches\\n')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Train DS: Image Size: 984 with 62 total batches\n","Test  DS: Image Size: 308 with 20 total batches\n","Valid DS: Image Size: 247 with 16 total batches\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P8724xlER1Ac"},"source":["## Validation Function"]},{"cell_type":"code","metadata":{"id":"RzFfrc7qW1J7","executionInfo":{"status":"ok","timestamp":1607880392685,"user_tz":-180,"elapsed":1825580,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["# Validation Method for the model\n","def validation(validation_data_loader, device, criterion, model):\n","    val_loss = 0\n","    correct_pixel = 0\n","    total_pixel = 0\n","\n","    for images, masks in validation_data_loader:\n","        images = images.to(device)\n","        masks = masks.type(torch.LongTensor)\n","        masks = masks.reshape(masks.shape[0], masks.shape[2], masks.shape[3])\n","        masks = masks.to(device)\n","        \n","        outputs = model(images)\n","        val_loss += criterion(outputs, masks).item()\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","        correct_pixel += (predicted == masks).sum().item()\n","        \n","        b, h, w = masks.shape\n","        batch_total_pixel = b * h * w\n","        \n","        total_pixel += batch_total_pixel\n","\n","    acc = correct_pixel/total_pixel\n","    return val_loss, acc"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvqT73-ladvu"},"source":["## TRAIN"]},{"cell_type":"markdown","metadata":{"id":"30W3QsK4R8vz"},"source":["### Network Model"]},{"cell_type":"code","metadata":{"id":"W7xv9Ej2Iv-3","executionInfo":{"status":"ok","timestamp":1607880402762,"user_tz":-180,"elapsed":1835646,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["model = UnetModel(number_of_classes, dropout_rate).to(device)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G591UjLxiIVs","executionInfo":{"status":"ok","timestamp":1607880403222,"user_tz":-180,"elapsed":1836097,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"57977461-8b52-4ced-d667-7d9f034a3a4f"},"source":["summary(model, input_size=(1, 256, 256))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 256, 256]             320\n","       BatchNorm2d-2         [-1, 32, 256, 256]              64\n","              ReLU-3         [-1, 32, 256, 256]               0\n","         Dropout2d-4         [-1, 32, 256, 256]               0\n","            Conv2d-5         [-1, 32, 256, 256]           9,248\n","       BatchNorm2d-6         [-1, 32, 256, 256]              64\n","              ReLU-7         [-1, 32, 256, 256]               0\n","         MaxPool2d-8         [-1, 32, 128, 128]               0\n","            Conv2d-9         [-1, 64, 128, 128]          18,496\n","      BatchNorm2d-10         [-1, 64, 128, 128]             128\n","             ReLU-11         [-1, 64, 128, 128]               0\n","        Dropout2d-12         [-1, 64, 128, 128]               0\n","           Conv2d-13         [-1, 64, 128, 128]          36,928\n","      BatchNorm2d-14         [-1, 64, 128, 128]             128\n","             ReLU-15         [-1, 64, 128, 128]               0\n","        MaxPool2d-16           [-1, 64, 64, 64]               0\n","           Conv2d-17          [-1, 128, 64, 64]          73,856\n","      BatchNorm2d-18          [-1, 128, 64, 64]             256\n","             ReLU-19          [-1, 128, 64, 64]               0\n","        Dropout2d-20          [-1, 128, 64, 64]               0\n","           Conv2d-21          [-1, 128, 64, 64]         147,584\n","      BatchNorm2d-22          [-1, 128, 64, 64]             256\n","             ReLU-23          [-1, 128, 64, 64]               0\n","        MaxPool2d-24          [-1, 128, 32, 32]               0\n","           Conv2d-25          [-1, 256, 32, 32]         295,168\n","      BatchNorm2d-26          [-1, 256, 32, 32]             512\n","             ReLU-27          [-1, 256, 32, 32]               0\n","        Dropout2d-28          [-1, 256, 32, 32]               0\n","           Conv2d-29          [-1, 256, 32, 32]         590,080\n","      BatchNorm2d-30          [-1, 256, 32, 32]             512\n","             ReLU-31          [-1, 256, 32, 32]               0\n","        MaxPool2d-32          [-1, 256, 16, 16]               0\n","           Conv2d-33          [-1, 512, 16, 16]       1,180,160\n","      BatchNorm2d-34          [-1, 512, 16, 16]           1,024\n","             ReLU-35          [-1, 512, 16, 16]               0\n","        Dropout2d-36          [-1, 512, 16, 16]               0\n","           Conv2d-37          [-1, 512, 16, 16]       2,359,808\n","      BatchNorm2d-38          [-1, 512, 16, 16]           1,024\n","             ReLU-39          [-1, 512, 16, 16]               0\n","UpsamplingNearest2d-40          [-1, 512, 32, 32]               0\n","           Conv2d-41          [-1, 256, 32, 32]       1,769,728\n","      BatchNorm2d-42          [-1, 256, 32, 32]             512\n","             ReLU-43          [-1, 256, 32, 32]               0\n","        Dropout2d-44          [-1, 256, 32, 32]               0\n","           Conv2d-45          [-1, 256, 32, 32]         590,080\n","      BatchNorm2d-46          [-1, 256, 32, 32]             512\n","             ReLU-47          [-1, 256, 32, 32]               0\n","UpsamplingNearest2d-48          [-1, 256, 64, 64]               0\n","           Conv2d-49          [-1, 128, 64, 64]         442,496\n","      BatchNorm2d-50          [-1, 128, 64, 64]             256\n","             ReLU-51          [-1, 128, 64, 64]               0\n","        Dropout2d-52          [-1, 128, 64, 64]               0\n","           Conv2d-53          [-1, 128, 64, 64]         147,584\n","      BatchNorm2d-54          [-1, 128, 64, 64]             256\n","             ReLU-55          [-1, 128, 64, 64]               0\n","UpsamplingNearest2d-56        [-1, 128, 128, 128]               0\n","           Conv2d-57         [-1, 64, 128, 128]         110,656\n","      BatchNorm2d-58         [-1, 64, 128, 128]             128\n","             ReLU-59         [-1, 64, 128, 128]               0\n","        Dropout2d-60         [-1, 64, 128, 128]               0\n","           Conv2d-61         [-1, 64, 128, 128]          36,928\n","      BatchNorm2d-62         [-1, 64, 128, 128]             128\n","             ReLU-63         [-1, 64, 128, 128]               0\n","UpsamplingNearest2d-64         [-1, 64, 256, 256]               0\n","           Conv2d-65         [-1, 32, 256, 256]          27,680\n","      BatchNorm2d-66         [-1, 32, 256, 256]              64\n","             ReLU-67         [-1, 32, 256, 256]               0\n","        Dropout2d-68         [-1, 32, 256, 256]               0\n","           Conv2d-69         [-1, 32, 256, 256]           9,248\n","      BatchNorm2d-70         [-1, 32, 256, 256]              64\n","             ReLU-71         [-1, 32, 256, 256]               0\n","           Conv2d-72          [-1, 2, 256, 256]              66\n","      BatchNorm2d-73          [-1, 2, 256, 256]               4\n","================================================================\n","Total params: 7,852,006\n","Trainable params: 7,852,006\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.25\n","Forward/backward pass size (MB): 496.50\n","Params size (MB): 29.95\n","Estimated Total Size (MB): 526.70\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yebdpoH6SD5c"},"source":["### Loss and Optimizer of the Model"]},{"cell_type":"code","metadata":{"id":"d_Wr6eA1SGQg","executionInfo":{"status":"ok","timestamp":1607880403223,"user_tz":-180,"elapsed":1836089,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["criterion = nn.CrossEntropyLoss()\r\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9LXpU3UgSHj8"},"source":["### Learning Rate Scheduler"]},{"cell_type":"code","metadata":{"id":"RjyRPQJFSKZR","executionInfo":{"status":"ok","timestamp":1607880403225,"user_tz":-180,"elapsed":1836082,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=6, verbose=True) # Works better and easier\r\n","# scheduler = StepLR(optimizer, step_size=2, gamma=0.2)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XH-nENVZSRy6"},"source":["### **Code**"]},{"cell_type":"code","metadata":{"id":"xh88Gku874Zi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607880899086,"user_tz":-180,"elapsed":2331934,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"86b4fad2-c4aa-4ec2-cbf5-61afa6daba65"},"source":["# Training of the Model\n","total_steps = len(train_data_loader)\n","print(f\"{epochs} Epochs & {total_steps} Total Steps per Epoch\")\n","\n","start_time = timer()\n","print(f'Training Started in {start_time} sec.')\n","model.train()\n","\n","batch_step = 0\n","for epoch in range(epochs):\n","    for i, (images, masks) in enumerate(train_data_loader, 1):\n","        images = images.to(device)  # Sends to GPU\n","        masks = masks.type(torch.LongTensor)\n","        masks = masks.reshape(masks.shape[0], masks.shape[2], masks.shape[3])\n","        masks = masks.to(device)    # Sends to GPU\n","\n","        # Forward pass\n","        predicts = model(images)\n","        loss = criterion(predicts, masks)\n","\n","        # This doubles our batch size\n","        if batch_extender:\n","            if batch_step == 0:\n","                optimizer.zero_grad()\n","                loss.backward()\n","                batch_step = 1\n","            elif batch_step == 1:\n","                loss.backward()\n","                optimizer.step()\n","                batch_step = 0\n","        else:\n","            # Backward and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        if i % int(total_steps/loss_print_per_epoch) == 0:\n","            if validation_on:\n","                acc = 0\n","                # Validation Part\n","                model.eval()\n","                with torch.no_grad():\n","                    validation_loss, validation_accuracy = validation(validation_data_loader, device, criterion, model)\n","                model.train()\n","            valstr = f'\\tValid. Loss: {(validation_loss/len(validation_data_loader)):.4f}\\tValid. Acc.: {validation_accuracy * 100:.3f}%' if validation_on else ''\n","            print(f'Epoch: {epoch + 1}/{epochs}\\tSt: {i}/{total_steps}\\tLast.Loss: {loss.item():4f}{valstr}')\n","    if scheduler_on:\n","        scheduler.step(acc) # -> ReduceLROnPlateau\n","\n","print('Execution time:', '{:5.2f}'.format(timer() - start_time), 'seconds')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["40 Epochs & 62 Total Steps per Epoch\n","Training Started in 2415.564549072 sec.\n","Epoch: 1/40\tSt: 62/62\tLast.Loss: 0.239379\tValid. Loss: 0.2271\tValid. Acc.: 93.445%\n","Epoch: 2/40\tSt: 62/62\tLast.Loss: 0.179410\tValid. Loss: 0.3607\tValid. Acc.: 86.899%\n","Epoch: 3/40\tSt: 62/62\tLast.Loss: 0.135170\tValid. Loss: 0.1483\tValid. Acc.: 94.071%\n","Epoch: 4/40\tSt: 62/62\tLast.Loss: 0.128391\tValid. Loss: 0.1702\tValid. Acc.: 93.152%\n","Epoch: 5/40\tSt: 62/62\tLast.Loss: 0.106309\tValid. Loss: 0.1238\tValid. Acc.: 95.064%\n","Epoch: 6/40\tSt: 62/62\tLast.Loss: 0.110958\tValid. Loss: 0.1233\tValid. Acc.: 95.125%\n","Epoch: 7/40\tSt: 62/62\tLast.Loss: 0.101182\tValid. Loss: 0.1248\tValid. Acc.: 94.673%\n","Epoch: 8/40\tSt: 62/62\tLast.Loss: 0.098810\tValid. Loss: 0.1442\tValid. Acc.: 94.191%\n","Epoch     8: reducing learning rate of group 0 to 2.0000e-03.\n","Epoch: 9/40\tSt: 62/62\tLast.Loss: 0.080561\tValid. Loss: 0.1199\tValid. Acc.: 95.079%\n","Epoch: 10/40\tSt: 62/62\tLast.Loss: 0.114220\tValid. Loss: 0.1022\tValid. Acc.: 95.833%\n","Epoch: 11/40\tSt: 62/62\tLast.Loss: 0.078569\tValid. Loss: 0.0981\tValid. Acc.: 95.940%\n","Epoch: 12/40\tSt: 62/62\tLast.Loss: 0.115963\tValid. Loss: 0.0955\tValid. Acc.: 96.093%\n","Epoch: 13/40\tSt: 62/62\tLast.Loss: 0.111904\tValid. Loss: 0.0959\tValid. Acc.: 96.092%\n","Epoch: 14/40\tSt: 62/62\tLast.Loss: 0.088949\tValid. Loss: 0.7292\tValid. Acc.: 68.680%\n","Epoch: 15/40\tSt: 62/62\tLast.Loss: 0.079562\tValid. Loss: 0.1750\tValid. Acc.: 93.929%\n","Epoch    15: reducing learning rate of group 0 to 4.0000e-04.\n","Epoch: 16/40\tSt: 62/62\tLast.Loss: 0.125756\tValid. Loss: 0.0862\tValid. Acc.: 96.508%\n","Epoch: 17/40\tSt: 62/62\tLast.Loss: 0.082157\tValid. Loss: 0.0866\tValid. Acc.: 96.514%\n","Epoch: 18/40\tSt: 62/62\tLast.Loss: 0.127495\tValid. Loss: 0.0847\tValid. Acc.: 96.569%\n","Epoch: 19/40\tSt: 62/62\tLast.Loss: 0.069449\tValid. Loss: 0.0852\tValid. Acc.: 96.597%\n","Epoch: 20/40\tSt: 62/62\tLast.Loss: 0.080746\tValid. Loss: 0.0832\tValid. Acc.: 96.626%\n","Epoch: 21/40\tSt: 62/62\tLast.Loss: 0.094685\tValid. Loss: 0.0830\tValid. Acc.: 96.673%\n","Epoch: 22/40\tSt: 62/62\tLast.Loss: 0.075024\tValid. Loss: 0.0836\tValid. Acc.: 96.623%\n","Epoch    22: reducing learning rate of group 0 to 8.0000e-05.\n","Epoch: 23/40\tSt: 62/62\tLast.Loss: 0.066178\tValid. Loss: 0.0812\tValid. Acc.: 96.750%\n","Epoch: 24/40\tSt: 62/62\tLast.Loss: 0.084428\tValid. Loss: 0.0803\tValid. Acc.: 96.766%\n","Epoch: 25/40\tSt: 62/62\tLast.Loss: 0.090834\tValid. Loss: 0.0812\tValid. Acc.: 96.736%\n","Epoch: 26/40\tSt: 62/62\tLast.Loss: 0.096092\tValid. Loss: 0.0809\tValid. Acc.: 96.753%\n","Epoch: 27/40\tSt: 62/62\tLast.Loss: 0.082429\tValid. Loss: 0.0803\tValid. Acc.: 96.761%\n","Epoch: 28/40\tSt: 62/62\tLast.Loss: 0.075847\tValid. Loss: 0.0806\tValid. Acc.: 96.771%\n","Epoch: 29/40\tSt: 62/62\tLast.Loss: 0.065246\tValid. Loss: 0.0803\tValid. Acc.: 96.774%\n","Epoch    29: reducing learning rate of group 0 to 1.6000e-05.\n","Epoch: 30/40\tSt: 62/62\tLast.Loss: 0.069501\tValid. Loss: 0.0801\tValid. Acc.: 96.815%\n","Epoch: 31/40\tSt: 62/62\tLast.Loss: 0.118663\tValid. Loss: 0.0806\tValid. Acc.: 96.758%\n","Epoch: 32/40\tSt: 62/62\tLast.Loss: 0.085215\tValid. Loss: 0.0794\tValid. Acc.: 96.809%\n","Epoch: 33/40\tSt: 62/62\tLast.Loss: 0.117432\tValid. Loss: 0.0794\tValid. Acc.: 96.794%\n","Epoch: 34/40\tSt: 62/62\tLast.Loss: 0.072195\tValid. Loss: 0.0797\tValid. Acc.: 96.819%\n","Epoch: 35/40\tSt: 62/62\tLast.Loss: 0.086177\tValid. Loss: 0.0795\tValid. Acc.: 96.810%\n","Epoch: 36/40\tSt: 62/62\tLast.Loss: 0.098770\tValid. Loss: 0.0790\tValid. Acc.: 96.824%\n","Epoch    36: reducing learning rate of group 0 to 3.2000e-06.\n","Epoch: 37/40\tSt: 62/62\tLast.Loss: 0.075024\tValid. Loss: 0.0789\tValid. Acc.: 96.810%\n","Epoch: 38/40\tSt: 62/62\tLast.Loss: 0.077185\tValid. Loss: 0.0798\tValid. Acc.: 96.798%\n","Epoch: 39/40\tSt: 62/62\tLast.Loss: 0.071679\tValid. Loss: 0.0791\tValid. Acc.: 96.815%\n","Epoch: 40/40\tSt: 62/62\tLast.Loss: 0.063481\tValid. Loss: 0.0786\tValid. Acc.: 96.818%\n","Execution time: 495.35 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qqpG2sT_ReGo","executionInfo":{"status":"ok","timestamp":1607880902030,"user_tz":-180,"elapsed":2334870,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}}},"source":["# Save the model\r\n","torch.save(model.state_dict(), trained_model_path)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0QeQt4ERge2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607880902034,"user_tz":-180,"elapsed":2334863,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"cb1bea49-6558-4c88-9372-e804c946a85e"},"source":["# Restore the model from \"model_check.pt\"\r\n","model = UnetModel(number_of_classes, dropout_rate).to(device)\r\n","\r\n","# Load to CPU. Later it can be moved to GPU as needed\r\n","model.load_state_dict(torch.load(trained_model_path, map_location=torch.device('cpu')))"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"HP8UwtvHSX_J"},"source":["## TEST"]},{"cell_type":"code","metadata":{"id":"3xtQu-8rWta_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607880904777,"user_tz":-180,"elapsed":2337597,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"c671e8bf-4ab4-44b6-a9fe-81b7d19893c4"},"source":["all_forms = []\n","all_predictions = []\n","view_count = 0\n","# Test the model\n","model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","with torch.no_grad():  # used for dropout layers\n","    correct_pixel = 0\n","    total_pixel = 0\n","    for images, masks in test_data_loader:\n","        images = images.to(device)\n","        masks = masks.type(torch.LongTensor)\n","        # delete color channel to compare directly with prediction\n","        masks = masks.reshape(masks.shape[0], masks.shape[2], masks.shape[3])\n","        masks = masks.to(device)\n","\n","        predicts = model(images)\n","        _, predicted = torch.max(predicts.data, 1)\n","        correct_pixel += (predicted == masks).sum().item()\n","\n","        b, h, w = masks.shape\n","        batch_total_pixel = b * h * w\n","        total_pixel += batch_total_pixel\n","        \n","\n","        # if pre-set addes images to list\n","        if is_saving_output:\n","            af, ap = undo_preprocess(images, predicts)\n","            all_forms.extend(af)\n","            all_predictions.extend(ap)\n","\n","\n","        # To observe random batch prediction uncomment!\n","        if sample_view and view_count < 10 and random.random() > 0.5:\n","            view_count += 1\n","            images, masks = undo_preprocess(images, predicts)\n","            plt_images(images, masks)\n","\n","    print(f\"{correct_pixel} / {total_pixel}\")\n","    print(f\"Test Accuracy on the model with {len(test_data_loader) * batch_size} images: {100 * correct_pixel / total_pixel:.4f}%\")\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["19571625 / 20185088\n","Test Accuracy on the model with 320 images: 96.9608%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wIYqvaxASbNE"},"source":["## View"]},{"cell_type":"code","metadata":{"id":"ZbvoGRiqKys5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607880909464,"user_tz":-180,"elapsed":2342275,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"dba0d4bc-efdc-49d6-96da-d3e01e2e14d7"},"source":["# Saves the output\r\n","if is_saving_output:\r\n","    save_predictions(np.array(all_forms), np.array(all_predictions))\r\n","\r\n","# # Gets the images and their predicted masks in normalized\r\n","# images, masks = undo_preprocess(images, predicts)\r\n","\r\n","# # Showing last batch as sample\r\n","# plt_images(images, masks)\r\n","\r\n","# # Saves the last batch as sample as input and output images\r\n","# # save_output_batch(images, masks)\r\n","\r\n","print(\"Program Finished!\")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["You can find predictions in '/content/drive/My Drive/PixelwiseCNN/output/prediction/'\n","Program Finished!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vmmUVrSEL6i5"},"source":["## Post Process"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"247u7lFyL5u4","executionInfo":{"status":"ok","timestamp":1607881212215,"user_tz":-180,"elapsed":3362,"user":{"displayName":"Onur Kirman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjRN2CZWW7mvlqHT90BiUrRHnhW9jMdY8M7uDHdZQ=s64","userId":"05711157021305569288"}},"outputId":"f5376f59-37ae-46c8-cdf4-5c45c2288750"},"source":["# %shell python boundingbox.py"],"execution_count":24,"outputs":[{"output_type":"stream","text":["'Boundingbox.py' Started!\n","100% 308/308 [00:02<00:00, 150.25it/s]\n","Program Finished!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":24}]}]}